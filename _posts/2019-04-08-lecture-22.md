---
layout: distill
title: "Lecture 22: Bayesian non-parametrics"
description: An introduction to Bayesian non-parametrics, including the Dirichlet process.
date: 2019-04-08

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Yao Chong Lim  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yue Wu
    url: "#"
  - name: Dongshunyi Li
    url: "#"
  - name: Steve Landers
    url: "#"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "#"  # optional URL to the editor's homepage

---

## Motivation via Clustering

## The Dirichlet distribution

## Parametric vs nonparametric

## The Dirichlet process

## Metaphors for the Dirichlet process

There are many ways to visualize the Dirichlet process. Three common metaphors are the Polya urn scheme, the Chinese restaurant process, and the stick-breaking process

### Pólya urn scheme

<figure id="polya-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/polya_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the Pólya urn scheme.
      </figcaption>
    </div>
  </div>
</figure>

<!-- The predictive distribution given by the Dirichlet process can be understood intuitively using the Polya urn scheme (also called the Blackwell-MacQueen urn scheme). -->

In the Pólya urn scheme (also called the Blackwell-MacQueen urn scheme)<d-cite key="blackwell1973"></d-cite>, imagine we have an urn initially containing a black ball of mass $\alpha$. To generate each sample $X_n$, sample a ball from the urn with probability proportional to its mass. The color of the ball is the partition that $X_n$ comes from. If the ball is black, choose a new previously unseen color for the sample, and return the black ball and a unit-mass ball of that new color to the urn. If the ball is not black, just return it and another unit-mass ball of the same color to the urn.

Then, after sampling $n$ balls, the joint distribution of the balls' colors (i.e. the cluster the balls belong to) follows a Dirichlet process.

From this scheme, we can also easily see the self-reinforcing property that the Dirichlet process exhibits: new samples are more likely to be drawn from partitions that already have many existing samples.

### The Chinese restaurant process

<!-- [image?] -->

The Chinese restaurant process is an equivalent description of the Pólya urn scheme.

Imagine a restaurant with an infinite number of tables, all initially empty. The first customer enters the restaurant, and picks a table to sit at. When the $n$-th customer enters, they either 1) sits at an existing table $k$ with probability $\frac{m_k}{n - 1 + \alpha}$, where $m_k$ is the number of people that are currently sitting at table $k$, or 2) starts a new table with probability $\frac{\alpha}{n - 1 + \alpha}$. We can see that this description is equivalent to the Pólya urn scheme above.

<!-- We can generalize the Chinese restaurant process as follows: The probability now depends on both the number of people  -->

### Note on Exchangeability

Even though the two descriptions above have a notion of an _order_ of the drawn samples, it turns out that the distribution over the partitions of the first $N$ samples does not _depend on the order of the samples_. However this does not mean the samples are independent, since we have seen the self-reinforcing property of the Dirichlet process - samples tend to be drawn from partitions that already have many existing samples.

Distributions that do not depend on the order of the samples are called __exchangeable__ distributions.

De Finetti's theorem states that if a sequence of observations are exchangeable, then there must exist a distribution given which the samples are i.i.d. Here, our observations are i.i.d. given the Dirichlet process.

### The stick-breaking process

<figure id="stick-breaking-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/stick_breaking_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the stick-breaking process.
      </figcaption>
    </div>
  </div>
</figure>

Another alternative view of the Dirichlet process is the stick-breaking process to construct the partitions of the DP<d-cite key="sethuraman1994"></d-cite>.

Imagine we begin with a stick of unit length, which represents our total probability. We then repeatedly break off fractions of the stick, and assign parameters to each broken off fraction according to our base measure.

Concretely, to construct the $k$-th partition:
1. Sample a Beta$(1, \alpha)$ random variable $\beta_k \in [0, 1]$.
1. Break off a fraction $\beta_k$ of the remaining stick. This gives us the $k$-th partition. We can calculate its atom size $\pi_k$ using the previous fractions $\beta_1, \dots, \beta_k$.
1. Sample a random parameter $\theta_k$ for the partition for this atom from our base measure $H$: $\theta_k \sim H$.
1. Recur on the remaining stick.

In summary,
<d-math block>
\begin{aligned}
\beta_k &\sim \text{Beta}(1, \alpha) \\
\pi_k &= \beta_k \prod_{\ell=1}^{k-1} (1 - \beta_{\ell})
\end{aligned} \\
\theta_k &\sim H \\
G &= \sum_{k=1}^{\infty} \pi_k \delta_{\theta_k}
</d-math>

The stick-breaking metaphor shows us how we can easily approximate a Dirichlet process. The final distribution $G$ is obtained from a weighted sum of the sampled paramters $\theta_k$, weighted by the atom size $\pi_k$. Since the atom sizes of the created partitions in the stick-breaking process are monotonically decreasing, we can get a good approximation of $G$ by just using the first $k$ sampled parameters and atom sizes.

